# Standard library imports
import asyncio
import logging

# å¯¼å…¥é…ç½®å’ŒæœåŠ¡
from app.config import HIDE_TOOLS_CONTENT
from app.llm_service import llm_service
from app.tool_manager.processor import ToolsProcessor

# é…ç½®æ¨¡å—çº§æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class Agent:
    """ä»£ç†ç±»ï¼Œç®¡ç†ä¸ç”¨æˆ·çš„å¯¹è¯å’Œå·¥å…·è°ƒç”¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä»£ç†"""
        self.system_prompt = self._get_system_prompt()
    
    def _get_system_prompt(self):
        """è·å–ç³»ç»Ÿæç¤º"""
        return """
        You are a professional, patient, knowledgeable and intelligent assistant.
        Your areas of expertise include Finance, Pharmaceuticals, Computer Science, and other professional domains.
        Users will ask you questions, and you can answer them.
        You can have normal conversations with users, answering whatever they ask, just remember you're a professional assistant.
        If you don't know the answer, it's okay to say you're not certain.

        ã€About Youã€‘
        Your role: A very smart Agent
        Your name: Users can give you one, otherwise you're just Smart Agent
        What you can do: You're an AI Agent with an LLM core that can use tools, with expertise in multiple domains including Finance, Pharmaceuticals, and Computer Science.
        Your strength is utilizing specialized tools to complete complex tasks in professional domains.
        Your current LLM core is: DeepSeek
        Your current tools library (TOOLS) is: OpenManus
        Languages you're proficient in: Chinese, English

        ã€About the Tools Library (TOOLS)ã€‘
        When replying to users, you can respond normally. Only when you think a task requires using the tools library (TOOLS) should you activate your tools.
        These tasks include but are not limited to:
        1. Using a browser to search for information (LLM cores have time limitations, but by searching yourself you can get the latest information)
        2. Operating a browser to do things (like looking up a file, etc.)
        3. Downloading files
        4. Performing professional analysis on files

        User messages are sent directly to you, but your replies go through a filter that can recognize key information and activate the tools library TOOLS.
        You can write the tool operations you want to perform in hidden statements.
        Your answers will contain hidden responses at the end of each dialogue segment, wrapped in special symbols.
        Hidden response format: [[TOOLS:TRUE/FALSE][hidden content]]
        Example responses:
        "Would you like to check the latest Tesla stock information? I can help you look that up. [[TOOLS:TRUE][Search for the latest Tesla stock information]]"
        "I know the answer to your question, no need for special search. [[TOOLS:FALSE][none]]"
        "I can help you search and download the paper Attention is All You Need [[TOOLS:TRUE][Use browser to search for the paper Attention is All You Need and find the pdf file and download it]]"
        "Let me help you organize Tesla's latest financial report [[TOOLS:TRUE][Search and download Tesla's latest financial report]]"
        "Your question is simple, it can be calculated using Excel. I'll help you calculate it. [[TOOLS:TRUE][Use Excel to calculate the user's requested data...]]"
        "I know the answer to your question, no need for special search. [[TOOLS:FALSE][none]]"

        Always include [[TOOLS STATUS][PROMPT]]
        Please always mark [[TOOLS STATUS][PROMPT]] at the end of your response
        For PROMPT with specific content, please include specific tool requests and the content of the request in the PROMPT.
        """
    
    async def chat_with_user(self, conversation, user_message, collect_all=False):
        """
        ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯çš„å¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°
        
        Parameters:
            conversation (list): å½“å‰å¯¹è¯å†å²
            user_message (str): ç”¨æˆ·å½“å‰è¾“å…¥çš„æ¶ˆæ¯
            collect_all (bool): æ˜¯å¦æ”¶é›†æ‰€æœ‰ä¸­é—´ç»“æœï¼Œç”¨äºéæµå¼å¤„ç†
            
        Generator Returns:
            tuple: (æ›´æ–°çš„å¯¹è¯å†å², è°ƒè¯•ä¿¡æ¯(æœªä½¿ç”¨), ç”ŸæˆçŠ¶æ€)
        """
        # å¦‚æœä¸éœ€è¦æ”¶é›†æ‰€æœ‰ç»“æœï¼Œå°±æ­£å¸¸ä½¿ç”¨ç”Ÿæˆå™¨
        if not collect_all:
            # å¤åˆ¶å¯¹è¯å†å²ä»¥é¿å…ä¿®æ”¹åŸå§‹åˆ—è¡¨
            updated_conv = list(conversation)
            
            # å°†ç”¨æˆ·çš„æ–°æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            updated_conv.append({"role": "user", "content": user_message})

            # æ„é€ å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæç¤ºå’Œå¯¹è¯å†å²
            messages = [{"role": "system", "content": self.system_prompt}] + updated_conv

            try:
                # è·å–æµå¼å“åº”
                partial_response = []
                
                async for content in llm_service.generate_streaming_response(messages):
                    if content:
                        # å°†æ–°å†…å®¹æ·»åŠ åˆ°éƒ¨åˆ†å“åº”ä¸­
                        partial_response.append(content)
                        current_text = "".join(partial_response)
                        
                        # å¤„ç†å¯¹è¯å†å²ï¼šå¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯æ¥è‡ªç”¨æˆ·ï¼Œæ·»åŠ åŠ©æ‰‹å›å¤ï¼›å¦åˆ™æ›´æ–°åŠ©æ‰‹å›å¤
                        if updated_conv[-1]["role"] == "user":
                            updated_conv.append({"role": "assistant", "content": current_text})
                        else:
                            updated_conv[-1]["content"] = current_text

                        # è¿”å›æ›´æ–°çš„å¯¹è¯å†å²å’Œç”ŸæˆçŠ¶æ€
                        yield updated_conv, "", True
            except Exception as e:
                # æ•è·APIè°ƒç”¨å¼‚å¸¸ï¼Œå°†é”™è¯¯æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²å¹¶è¿”å›
                updated_conv.append({"role": "assistant", "content": f"Smart Agent: Interface call exception: {e}"})
                yield updated_conv, "", False  # è¿”å›æ›´æ–°çš„å¯¹è¯å†å²ï¼Œæ— è°ƒè¯•ä¿¡æ¯ï¼Œéç”ŸæˆçŠ¶æ€
                return
            
            # å®Œæ•´çš„LLMå“åº”
            final_response = "".join(partial_response)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†å·¥å…·
            if "[[TOOLS:TRUE" in final_response:
                # æå–å·¥å…·æŒ‡ä»¤
                cleaned_message, tools_status, tools_content = ToolsProcessor.extract_tools_content(final_response)
                
                if tools_status:
                    # æ˜¾ç¤ºå·¥å…·å¤„ç†å·²å¼€å§‹
                    if updated_conv[-1]["role"] == "assistant":
                        updated_conv[-1]["content"] = f"{cleaned_message}\n\n[Tool Processing Started...]"
                    else:
                        updated_conv.append({"role": "assistant", "content": f"{cleaned_message}\n\n[Tool Processing Started...]"})
                    
                    # è¿”å›æ›´æ–°ï¼Œæ˜¾ç¤ºå·¥å…·å¤„ç†å·²å¼€å§‹
                    yield updated_conv, "", True
                    
                    # è®¾ç½®è¿›åº¦è·Ÿè¸ª
                    progress_updates = []
                    
                    # è¿›åº¦å›è°ƒå‡½æ•°
                    def progress_handler(message):
                        progress_updates.append(message)
                        # å®æ—¶æ›´æ–°å¯¹è¯ä¸­çš„è¿›åº¦
                        if updated_conv[-1]["role"] == "assistant":
                            current_content = updated_conv[-1]["content"]
                            if "[Tool Processing Progress]" not in current_content:
                                updated_content = f"{current_content}\n\n[Tool Processing Progress]\n{message}"
                            else:
                                parts = current_content.split("[Tool Processing Progress]")
                                updated_content = f"{parts[0]}[Tool Processing Progress]{parts[1]}\n{message}"
                            
                            updated_conv[-1]["content"] = updated_content
                            # æˆ‘ä»¬ä¸èƒ½ç›´æ¥åœ¨è¿™é‡Œyield - æˆ‘ä»¬å°†å­˜å‚¨æ›´æ–°å¹¶ç¨åå¤„ç†å®ƒä»¬
                    
                    # å¤„ç†å¸¦æœ‰è¿›åº¦è·Ÿè¸ªçš„å·¥å…·è¯·æ±‚
                    tools_result = await ToolsProcessor.process_tools_request_async_with_progress(
                        tools_content, progress_callback=progress_handler
                    )
                    
                    # å·¥å…·æ‰§è¡Œåï¼Œç”¨æœ€æ–°è¿›åº¦æ›´æ–°å¯¹è¯
                    if updated_conv[-1]["role"] == "assistant":
                        current_content = updated_conv[-1]["content"]
                        if "[Tool Processing Progress]" in current_content:
                            progress_text = "\n".join(progress_updates)
                            parts = current_content.split("[Tool Processing Progress]")
                            updated_content = f"{parts[0]}[Tool Processing Progress]\n{progress_text}\n\n[Tool Execution Complete]"
                        else:
                            progress_text = "\n".join(progress_updates)
                            updated_content = f"{current_content}\n\n[Tool Processing Progress]\n{progress_text}\n\n[Tool Execution Complete]"
                        
                        updated_conv[-1]["content"] = updated_content
                        yield updated_conv, "", True
                    
                    # ç°åœ¨ç”Ÿæˆå·¥å…·ç»“æœçš„æ‘˜è¦
                    summary_prompt = f"""
                    You have just used tools to accomplish a task for the user. 
                    Here is the original request: "{user_message}"
                    
                    The tools executed with the following result:
                    {tools_result}
                    
                    Please provide a clear, concise, and helpful summary of:
                    1. What tools were used and why
                    2. What was accomplished
                    3. Key findings or insights from the tool execution
                    4. Any relevant next steps or recommendations
                    
                    Format your response as a professional summary that focuses on the most important information.
                    """
                    
                    # æ·»åŠ æ‘˜è¦è¯·æ±‚åˆ°æ¶ˆæ¯ä¸­
                    summary_messages = [{"role": "system", "content": self.system_prompt}] + [
                        {"role": "assistant", "content": cleaned_message},
                        {"role": "user", "content": summary_prompt}
                    ]
                    
                    # æ›´æ–°UIä»¥æ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆæ‘˜è¦
                    if updated_conv[-1]["role"] == "assistant":
                        current_content = updated_conv[-1]["content"]
                        updated_conv[-1]["content"] = f"{current_content}\n\n[Generating Summary...]"
                        yield updated_conv, "", True
                    
                    try:
                        # è·å–æ‘˜è¦
                        final_summary = await llm_service.generate_response(summary_messages)
                        
                        # æ›´æ–°å¯¹è¯ä¸­çš„æ‘˜è¦
                        current_content = updated_conv[-1]["content"]
                        if "[Generating Summary...]" in current_content:
                            parts = current_content.split("[Generating Summary...]")
                            updated_conv[-1]["content"] = f"{parts[0]}\n\n[Tool Execution Summary]\n{final_summary}"
                        else:
                            updated_conv[-1]["content"] = f"{current_content}\n\n[Tool Execution Summary]\n{final_summary}"
                        
                        yield updated_conv, "", True
                        
                        # ä½¿ç”¨å·¥å…·ç»“æœå’Œæ‘˜è¦æ ¼å¼åŒ–æœ€ç»ˆå¤„ç†çš„å“åº”
                        processed_response = f"""
{cleaned_message}

[Tool Execution Complete]

[Tool Execution Summary]
{final_summary}
"""
                        
                    except Exception as e:
                        logger.error(f"Error generating summary: {e}")
                        # å¦‚æœæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ›´ç®€å•çš„æ ¼å¼
                        processed_response = f"""
{cleaned_message}

[Tool Execution Complete]

[Tool Results]
{tools_result}

[Note: Summary generation failed - {str(e)}]
"""
                else:
                    # å¦‚æœæ²¡æœ‰ä½¿ç”¨å·¥å…·ï¼ˆFALSEï¼‰ï¼Œåªæ˜¯æ­£å¸¸å¤„ç†æ¶ˆæ¯
                    processed_response = ToolsProcessor.process_message(final_response)
            else:
                # å¦‚æœæ ¹æœ¬æ²¡æœ‰å·¥å…·æ ‡è®°ï¼Œç…§åŸæ ·ä½¿ç”¨å“åº”
                processed_response = final_response
            
            # ç”¨å¤„ç†åçš„å“åº”æ›´æ–°æœ€ç»ˆçš„åŠ©æ‰‹æ¶ˆæ¯
            if updated_conv[-1]["role"] == "assistant":
                updated_conv[-1]["content"] = processed_response
            else:
                updated_conv.append({"role": "assistant", "content": processed_response})
            
            logger.info(f"ğŸ§¾ processed_response = {processed_response[:100]}..." if len(processed_response) > 100 else f"ğŸ§¾ processed_response = {processed_response}")
            logger.info(f"ğŸ“ updated_conv[-1] = {updated_conv[-1]}")
            logger.info(f"ğŸ“¤ yield to frontend: {updated_conv[-1]['content'][:100]}..." if len(updated_conv[-1]['content']) > 100 else f"ğŸ“¤ yield to frontend: {updated_conv[-1]['content']}")
            
            # æœ€ç»ˆè¿”å›å®Œæ•´çš„å¯¹è¯å†å²ï¼Œç”Ÿæˆç»“æŸ
            yield updated_conv, "", False
        else:
            # æ”¶é›†æ¨¡å¼ï¼šè¿è¡Œæ‰€æœ‰ç”Ÿæˆæ­¥éª¤ï¼Œä½†åªè¿”å›æœ€ç»ˆç»“æœ
            results = []
            async for result in self.chat_with_user(conversation, user_message, collect_all=False):
                results.append(result)
            
            if results:
                # åªè¿”å›æœ€ç»ˆç»“æœ
                yield results[-1]
            else:
                # å¦‚æœæ²¡æœ‰ç»“æœï¼Œè¿”å›åˆå§‹çŠ¶æ€
                updated_conv = list(conversation)
                updated_conv.append({"role": "user", "content": user_message})
                yield updated_conv, "", False

# åˆ›å»ºä»£ç†å®ä¾‹
agent = Agent()

# å¯¼å‡ºå…¬å…±æ¥å£
__all__ = ['Agent', 'agent']